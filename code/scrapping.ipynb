{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrappy_match_results(year, month, day, req_headers, folder_scrapped, file_match_result): \n",
    "    df_atp_filled_full = pd.read_csv(f'{folder_scrapped}\\\\{file_match_result}', sep='|')\n",
    "    \n",
    "    list_by_row = []\n",
    "    # for year in list_year:\n",
    "    #     for month in list_month:\n",
    "    #         for day in list_day:\n",
    "                urls = f'https://www.tennisexplorer.com/results/?type=atp-single&year={year}&month={month}&day={day}'\n",
    "                r1 = requests.get(urls, headers = req_headers, verify=False)\n",
    "                print(r1)\n",
    "                r1 = BeautifulSoup(r1.content, 'lxml')\n",
    "                r1 = r1.findAll('tr')\n",
    "                n_date = f'{year}-{month}-{day}'\n",
    "                for i in range(len(r1)):\n",
    "                    try:\n",
    "                        td_element = r1[i].findAll('td')\n",
    "                        \n",
    "                        if td_element[0].get('class') == ['t-name']:\n",
    "                            td_element = td_element[0]\n",
    "                            dict_temp = {}\n",
    "                            url_match = np.nan\n",
    "                            pl1_bet = np.nan\n",
    "                            pl2_bet = np.nan\n",
    "                            a_tag = td_element.find('a')\n",
    "                            a_text = a_tag.get_text(strip=True)\n",
    "                            a_href = a_tag.get('href')\n",
    "                            if 'player' in a_href:\n",
    "                                dict_temp.update({'Date':n_date,\n",
    "                                                'Location':np.nan,\n",
    "                                                'Info':url_match,\n",
    "                                                'Player':a_text,\n",
    "                                                'url':a_href,\n",
    "                                                'Bet_pl1':pl1_bet,\n",
    "                                                'Bet_pl2':pl2_bet})\n",
    "                            else:\n",
    "                                dict_temp.update({'Date':n_date,\n",
    "                                                'Location':a_text,\n",
    "                                                'Info':url_match,\n",
    "                                                'Player':'Field',\n",
    "                                                'url':a_href,\n",
    "                                                'Bet_pl1':pl1_bet,\n",
    "                                                'Bet_pl2':pl2_bet})\n",
    "                                \n",
    "                            list_by_row.append(dict_temp)\n",
    "                                \n",
    "                        else:\n",
    "                            url_match = td_element[11].find('a').get('href')\n",
    "                            pl1_bet = td_element[8]\n",
    "                            pl2_bet = td_element[9]\n",
    "                            td_element = td_element[1]\n",
    "                            dict_temp = {}\n",
    "                            a_tag = td_element.find('a')\n",
    "                            a_text = a_tag.get_text(strip=True)\n",
    "                            a_href = a_tag.get('href')\n",
    "                            \n",
    "                            if 'player' in a_href:\n",
    "                                dict_temp.update({'Date':n_date,\n",
    "                                                'Location':np.nan,\n",
    "                                                'Info':url_match,\n",
    "                                                'Player':a_text,\n",
    "                                                'url':a_href,\n",
    "                                                'Bet_pl1':pl1_bet,\n",
    "                                                'Bet_pl2':pl2_bet})\n",
    "                            else:\n",
    "                                dict_temp.update({'Date':n_date,\n",
    "                                                'Location':a_text,\n",
    "                                                'Info':url_match,\n",
    "                                                'Player':'Field',\n",
    "                                                'url':a_href,\n",
    "                                                'Bet_pl1':pl1_bet,\n",
    "                                                'Bet_pl2':pl2_bet})\n",
    "                                \n",
    "                            list_by_row.append(dict_temp)\n",
    "                    except:\n",
    "                        continue\n",
    "                    \n",
    "    \n",
    "    df_atp = pd.DataFrame(list_by_row)\n",
    "    df_atp['ply'] = np.where(df_atp['Player']=='Field', \"Field\", np.where(df_atp['Info'].isna(),'p2','p1'))\n",
    "    df_atp_filled =  df_atp.fillna(method='ffill')\n",
    "\n",
    "    not_link = df_atp_filled[df_atp_filled['ply']=='Field'].groupby(['Info'])[['Info']].count()\n",
    "    not_link = not_link[(not_link['Info']>1)]\n",
    "    not_link = not_link.index.unique().tolist()\n",
    "\n",
    "    df_atp_filled = df_atp_filled[~(df_atp_filled['Info'].isin(not_link))]\n",
    "    \n",
    "    df_atp_filled_full = pd.concat([df_atp_filled_full, df_atp_filled], axis = 0)\n",
    "    df_atp_filled_full.to_csv(f'{folder_scrapped}\\\\{file_match_result}', index=False, sep='|')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df_atp_filled, df_atp_filled_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrappy_fields_info(df_atp_filled, req_headers, folder_scrapped, file_fields_scrapped):\n",
    "    df_fields = df_atp_filled[(df_atp_filled['Player'] == 'Field')]\n",
    "    df_fields['url'] = df_fields[\"url\"].str.split(\"/\",expand=True)[1]\n",
    "    df_fields['url'] = 'https://www.tennisexplorer.com/'+ df_fields['url']\n",
    "    df_fields = df_fields[['Location','url']].drop_duplicates()\n",
    "    \n",
    "    df_field_surface_full = pd.read_csv(f'{folder_scrapped}\\\\{file_fields_scrapped}', sep='|')\n",
    "    df_fields = df_fields[~(df_fields['url'].isin(df_field_surface_full['url'].unique().tolist()))]\n",
    "    \n",
    "    df_field_surface = pd.DataFrame()\n",
    "    for url_fld in  df_fields['url'].unique().tolist():\n",
    "        df_field_temp = df_fields[(df_fields['url'] == url_fld)]\n",
    "        f1 = requests.get(url_fld, headers = req_headers, verify=False)\n",
    "        f1 = BeautifulSoup(f1.content, 'lxml')\n",
    "        f1 = f1.findAll('div', class_='box boxBasic lGray')[1].get_text()\n",
    "        f1 = f1.split(\",\")[-2].replace(\" \",\"\")\n",
    "        df_field_temp['Surface'] = f1\n",
    "        df_field_surface = pd.concat([df_field_surface, df_field_temp], axis = 0)\n",
    "        \n",
    "    df_field_surface_full = pd.concat([df_field_surface_full, df_field_surface], axis = 0)\n",
    "    df_field_surface_full.to_csv(f'{folder_scrapped}\\\\{file_fields_scrapped}', index=False, sep='|')\n",
    "    \n",
    "    return df_field_surface_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrappy_players_description(df_atp_filled, req_headers, folder_scrapped, file_players_desc):\n",
    "    df_games = df_atp_filled[~(df_atp_filled['Player'] == 'Field')]\n",
    "    df_games['url'] = 'https://www.tennisexplorer.com'+ df_games['url']\n",
    "    df_players = df_games[['Player','url']].drop_duplicates()\n",
    "\n",
    "    df_players_desc_full = pd.read_csv(f'{folder_scrapped}\\\\{file_players_desc}', sep='|')\n",
    "    df_players = df_players[~(df_players['url'].isin(df_players_desc_full['url'].unique().tolist()))]\n",
    "    \n",
    "    df_players_desc = pd.DataFrame()\n",
    "    # for ply_url in [df_players['url'].unique().tolist()[0]]:\n",
    "    for ply_url in df_players['url'].unique().tolist():\n",
    "        df_players_temp = df_players[(df_players['url'] == ply_url)]\n",
    "        p0 = requests.get(ply_url, headers = req_headers, verify=False)\n",
    "        p0 = BeautifulSoup(p0.content, 'lxml')\n",
    "        p1 = p0.findAll('div', class_='box boxBasic lGray')[1]\n",
    "        \n",
    "        for i in range(len(p1.findAll('div', class_=\"date\"))):\n",
    "            # print(p1.findAll('div', class_=\"date\")[i].get_text())\n",
    "            if 'Country' in p1.findAll('div', class_=\"date\")[i].get_text():\n",
    "                df_players_temp['flag'] = p1.findAll('div', class_=\"date\")[i].get_text().split(\":\")[-1].replace(\" \",\"\")\n",
    "            elif 'Age' in p1.findAll('div', class_=\"date\")[i].get_text():\n",
    "                df_players_temp['age'] = p1.findAll('div', class_=\"date\")[i].get_text().split(\":\")[-1].replace(\" \",\"\").split(\"(\")[0].replace(')','')\n",
    "            elif 'Plays' in p1.findAll('div', class_=\"date\")[i].get_text():\n",
    "                df_players_temp['hand'] = p1.findAll('div', class_=\"date\")[i].get_text().split(\":\")[-1].replace(\" \",\"\")\n",
    "            elif 'Weight' in p1.findAll('div', class_=\"date\")[i].get_text():\n",
    "                try:\n",
    "                    df_players_temp['height'] = p1.findAll('div', class_=\"date\")[i].get_text().split(\":\")[-1].replace(\" \",\"\").split(\"/\")[0].replace('cm','')\n",
    "                    df_players_temp['weight'] = p1.findAll('div', class_=\"date\")[1].get_text().split(\":\")[-1].replace(\" \",\"\").split(\"/\")[1].replace('kg','')\n",
    "                except:\n",
    "                    df_players_temp['height'] = np.NAN\n",
    "                    df_players_temp['weight'] = np.NAN\n",
    "                    \n",
    "        p2 = p0.findAll('div', class_='box lGray')[2]\n",
    "        df_players_temp['year_pro'] = min([p2.findAll(\"td\", class_ = 'year')[i].get_text() for i in range(len(p2.findAll(\"td\", class_ = 'year')))])\n",
    "        df_players_desc = pd.concat([df_players_desc, df_players_temp], axis = 0)\n",
    "        \n",
    "    df_players_desc_full = pd.concat([df_players_desc_full, df_players_desc], axis = 0)\n",
    "    df_players_desc_full.to_csv(f'{folder_scrapped}\\\\{file_players_desc}', index=False, sep='|')\n",
    "    \n",
    "    return df_games, df_players_desc_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrappy_games(df_games, df_players_desc, df_field_surface, folder_scrapped, file_games, input_path, daily_dump_folder):\n",
    "    \n",
    "    df_games_acum = pd.read_csv(f'{folder_scrapped}\\\\{file_games}', sep='|')\n",
    "    df_games = df_games.merge(df_players_desc, on = ['Player','url'], how = 'left')\n",
    "    \n",
    "    df_games_full = pd.DataFrame()\n",
    "    for info in df_games.Info.unique().tolist():\n",
    "        df_games_temp = df_games[(df_games['Info'] == info)]\n",
    "        df_games_hz = df_games_temp[['Location','Info','Bet_pl1','Bet_pl2']].drop_duplicates()\n",
    "        df_pl1_temp = df_games_temp[(df_games_temp['ply'] == 'p1')]\n",
    "        df_pl2_temp = df_games_temp[(df_games_temp['ply'] == 'p2')]\n",
    "        df_games_hz['Winner'] = df_pl1_temp['Player'].iloc[0]\n",
    "        df_games_hz['pl1_flag'] = df_pl1_temp['flag'].iloc[0]\n",
    "        df_games_hz['pl1_year_pro'] = df_pl1_temp['year_pro'].iloc[0]\n",
    "        df_games_hz['pl1_weight'] = df_pl1_temp['weight'].iloc[0]\n",
    "        df_games_hz['pl1_height'] = df_pl1_temp['height'].iloc[0]\n",
    "        df_games_hz['pl1_hand'] = df_pl1_temp['hand'].iloc[0]\n",
    "        df_games_hz['pl1_age'] = df_pl1_temp['age'].iloc[0]\n",
    "        df_games_hz['Loser'] = df_pl2_temp['Player'].iloc[0]\n",
    "        df_games_hz['pl2_flag'] = df_pl2_temp['flag'].iloc[0]\n",
    "        df_games_hz['pl2_year_pro'] = df_pl2_temp['year_pro'].iloc[0]\n",
    "        df_games_hz['pl2_weight'] = df_pl2_temp['weight'].iloc[0]\n",
    "        df_games_hz['pl2_height'] = df_pl2_temp['height'].iloc[0]\n",
    "        df_games_hz['pl2_hand'] = df_pl2_temp['hand'].iloc[0]\n",
    "        df_games_hz['pl2_age'] = df_pl2_temp['age'].iloc[0]\n",
    "        df_games_full = pd.concat([df_games_full, df_games_hz], axis = 0)\n",
    "        \n",
    "    df_games_full = df_games_full.merge(df_field_surface[['Location','Surface']], on = 'Location', how = 'left')\n",
    "    df_games_full.drop(columns='Info', inplace=True)\n",
    "    df_games_full.to_csv(os.path.join(input_path, daily_dump_folder, f'df_games_{alphonse}.csv'), index=False, sep='|')\n",
    "    \n",
    "    df_games_acum = pd.concat([df_games_acum, df_games_full], axis = 0)\n",
    "    df_games_acum.to_csv(f'{folder_scrapped}\\\\{file_games}', index=False, sep='|')\n",
    "    \n",
    "    return df_games_full, df_games_acum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_year =  ['0'+str(x) if len(str(x)) < 2 else str(x) for x in list(range(2022,2023))]\n",
    "list_month = ['0'+str(x) if len(str(x)) < 2 else str(x) for x in list(range(3,4))]\n",
    "list_day = ['0'+str(x) if len(str(x)) < 2 else str(x) for x in list(range(1,3))]\n",
    "req_headers = {\n",
    "'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "'accept-encoding': 'gzip, deflate, br',\n",
    "'accept-language': 'en-US,en;q=0.8',\n",
    "'upgrade-insecure-requests': '1',\n",
    "'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
    "}\n",
    "for year in list_year:\n",
    "    for month in list_month:\n",
    "        for day in list_day:\n",
    "df_atp_filled = scrappy_match_results(list_year, list_month, list_day, req_headers)\n",
    "df_field_surface = scrappy_fields_info(df_atp_filled, req_headers, folder_scrapped, file_fields_scrapped)\n",
    "df_games, df_players_desc = scrappy_players_description(df_atp_filled, req_headers, folder_scrapped, file_players_desc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_sports_ben",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
